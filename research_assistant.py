# -*- coding: utf-8 -*-
"""Research_assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCnFD-0qgfDNaKVwyuC6VWbZymDvg0bJ
"""

!pip install -U langchain_community tiktoken langchain-groq langchainhub chromadb langchain langgraph langchain_huggingface
!pip install InstructorEmbedding faiss-cpu PyPDF2 sentence-transformers==2.2.2

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS
from PyPDF2 import PdfReader
from langchain.schema import Document


def pdf_handler_with_store(file_path):

  text = ''
  pdf_reader = PdfReader(file_path)
  for page in pdf_reader.pages:
    text += page.extract_text()
  final_text = Document(page_content = text)
  return final_text

#Router
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

#data model
class RouteQuery(BaseModel):
  """Route a user query to the most relevant datasource."""
  datasource: Literal["vectorstore", "wiki_search"] = Field(
        ...,
        description="Given a user question choose to route it to wikipedia or a vectorstore or research.",
    )

from google.colab import userdata
# LLM with function call
from langchain_groq import ChatGroq
import os
groq_api_key=userdata.get('GROQ_API_KEY')
os.environ["GROQ_API_KEY"]=groq_api_key
llm=ChatGroq(groq_api_key=groq_api_key,model_name="Gemma2-9b-It")
structured_llm_router = llm.with_structured_output(RouteQuery)

# Prompt
system = """You are an expert at routing a user question to a vectorstore or wikipedia or research.
go to the vectorstore in case the user prompt mentions like from the pdf, pdf provide , based on context. In case user question mention like write sop, abstract, or any thing related to a paper go to research. Otherwise, use wiki-search."""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
question_router = route_prompt | structured_llm_router
'''
print(
    question_router.invoke(
        {"question": "who is Sharukh Khan?"}
    )
)
print(question_router.invoke({"question": "What are the types of agent memory?"}))
'''

!pip install wikipedia arxiv

### Working With Tools
from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper
from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun

## Arxiv and wikipedia Tools
arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)
arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)

api_wrapper=WikipediaAPIWrapper(top_k_results=1)
wiki=WikipediaQueryRun(api_wrapper=api_wrapper)

## Graph

from typing import List

from typing_extensions import TypedDict


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
    """

    question: str
    generation: str
    documents: List[str]

from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS
from PyPDF2 import PdfReader
from langchain.schema import Document


def pdf_handler_with_store(file_path):

  text = ''
  pdf_reader = PdfReader(file_path)
  for page in pdf_reader.pages:
    text += page.extract_text()


  text_splitter = CharacterTextSplitter(
        separator= "\n" ,
        chunk_size=1000,
        chunk_overlap=200,
        length_function= len
        )
  chunks = text_splitter.split_text(text)
  embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
  vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)
  return vectorstore

path = '/content/sodapdf-converted.pdf'

vectorstore = pdf_handler_with_store(path)

retriever = vectorstore.as_retriever()

def retrieve(state):
    """
    Retrieve documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]
    def list_to_document(content_list):
        text = ''
        for element in content_list:
          text += str(element) + ' '  # Add space as separator
        return Document(page_content=text)

    # Retrieval
    take  = retriever.invoke(question)
    documents = list_to_document(take)
    return {"documents": documents, "question": question}

def wiki_search(state):
    """
    wiki search based on the re-phrased question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with appended web results
    """

    print("---wikipedia---")
    print("---HELLO--")
    question = state["question"]
    print(question)

    # Wiki search
    docs = wiki.invoke({"query": question})
    #print(docs["summary"])
    wiki_results = docs
    wiki_results = Document(page_content=wiki_results)

    return {"documents": wiki_results, "question": question}

def research(state):
# LLM with function call
  from langchain_groq import ChatGroq
  groq_api_key='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'
  llm=ChatGroq(groq_api_key=groq_api_key,model_name="llama3-8b-8192")
  question = state["question"]
  result = llm.invoke(state['question'])
  output = Document(page_content=result.content)
  return {"documents": output, "question": question}

### Edges ###


'''def route_question(state):
    """
    Route question to wiki search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]
    source = question_router.invoke({"question": question})
    if source.datasource == "wiki_search":
        print("---ROUTE QUESTION TO Wiki SEARCH---")
        return "wiki_search"
    elif source.datasource == "research":
        print("---ROUTE QUESTION TO research-llm---")
        return "research"
    elif source.datasource == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"'''
### Edges ###

def route_question(state):
    """
    Route question to wiki search, RAG, or research-llm.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]

    if "research" in question.lower():
        print("---ROUTE QUESTION TO research-llm---")
        return "research"
    elif "pdf" in question.lower() or "context" in question.lower():  # Example condition for vectorstore - adjust as needed
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"
    else:
        print("---ROUTE QUESTION TO Wiki SEARCH---")
        return "wiki_search"

from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)
# Define the nodes
workflow.add_node("wiki_search", wiki_search)  # web search
workflow.add_node("research", research)
workflow.add_node("retrieve", retrieve) # retrieve

# Build graph
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "research": "research",
        "vectorstore": "retrieve",
    },
)
workflow.add_edge( "retrieve", END)
workflow.add_edge( "research", END)
workflow.add_edge( "wiki_search", END)
# Compile
app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

from pprint import pprint

# Run
inputs = {
    "question": "What is agent?"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'].page_content)

from pprint import pprint

# Run
inputs = {
    "question": "write a sop for the research on fine tuning llm for medical use cases"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'].page_content)

from pprint import pprint

# Run
inputs = {
    "question": "provide the content from the pdf"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'].page_content)

